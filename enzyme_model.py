# -*- coding: utf-8 -*-
"""Novoenzymes_Model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AQoqmtp2extawVBZyXMoXHAgF7lE9ysV
"""
from tqdm import tqdm
import copy
import numpy as np

import torch
from transformers import BertTokenizer, BertModel, BertConfig
from torchmetrics import SpearmanCorrCoef

# ==== Model ====

class Bert_MLPHead(torch.nn.Module):
  def __init__(self, pretrained_bert, add_bert_pooler = True, 
               include_ph = True, linear_layers = None, fusion ="mean"):
    super(Bert_MLPHead, self).__init__()
    """
    - pretrained_bert: Name of the pretrained bert.
    - add_bertpooler: If true, the last hidden state of the [cls] token will go 
                      through a linear layer with a tanh activation. The output
                      will be the sole output of the model.
    - include_ph: If true, the ph will be added to the last protein embedding
                  as an additional neuron
    - linear_layers: List of additional linear layers relative size. 
                      Example: [1,0.5] means add 2 intermediate layers of size
                      equal to the input size and half of the input size 
                      respectively.
    - fusion: Type of fusion acceptable values ["mean", "weighted"].
    """
    assert pretrained_bert is not None, "Pretrained_ber_name is required."
    # Load pretrained bert
    self.add_bert_pooler = add_bert_pooler
    self.bert = BertModel.from_pretrained(pretrained_bert, 
                                          add_pooling_layer = self.add_bert_pooler)
    in_shape = self.bert.config.hidden_size
    
    # Add neuron for pH in the first layer of the MLP head
    self.include_ph = include_ph
    if self.include_ph:
      in_shape += 1

    # Classifier
    if linear_layers is not None:
      # Stack intermediate layers
      layers = []
      for i, l in enumerate(linear_layers):
        out_shape = int(in_shape * l)
        assert out_shape > 0, (f"Choice of the layer {i} shape {l} is " +  
                              "causing a number of zero neurons.")
        layers.append(torch.nn.Linear(in_features= in_shape,
                                    out_features= out_shape))
        in_shape = out_shape
      layers.append(torch.nn.Linear(in_features= in_shape, out_features= 1))
      self.linear = torch.nn.Sequential(*layers)
    # One layer classifier
    else:
      self.linear = torch.nn.Sequential(
          torch.nn.Linear(in_features= in_shape, out_features= in_shape//2),
          torch.nn.ReLU(inplace = True),
          torch.nn.Linear(in_features= in_shape//2, out_features= 1))

    # Attention Fusion
    self.fusion = fusion

    if self.fusion == "weighted":
      in_shape = self.bert.config.hidden_size
      self.linear_weights = torch.nn.Linear(in_features=in_shape, 
                                            out_features = 1)
    
  def forward(self, input_ids, attention_mask, ph_values = None, 
              n_splits = None, position_ids = None):
    # Extract features 
    hidden_states = self.bert(input_ids, attention_mask, 
                              position_ids = position_ids,
                              return_dict = False)

    # With no pooling consider only the [cls] representation
    if self.add_bert_pooler:
      cls_states = hidden_states[1]
    else:
      cls_states = hidden_states[0][:, 0]

    # Group cls_state from same protein
    if n_splits is not None:
      cls_states_splits = cls_states.split(n_splits)
      # Fuse states 
      if self.fusion == "mean":
        cls_states = torch.stack([states.mean(dim = 0) 
        for states in cls_states_splits])

      elif self.fusion == "weighted":
        
        cls_states = []
        for states in cls_states_splits:
          
          if len(states) == 1:
            cls_states.append(states[0])
            continue

          weights = self.linear_weights(states)
          weights = torch.nn.functional.softmax(weights, dim = 0)
          cls_states.append((weights * states).sum(dim = 0))

        cls_states = torch.stack(cls_states)

    # Add the pH information to the protein embedding
    if self.include_ph:
      assert ph_values is not None, ("Include pH is activated " + 
                                        "but no pH values are given")

      cls_states = torch.cat((cls_states, ph_values.unsqueeze(1)), 
                              dim = 1)
    
    # Linear 
    output = self.linear(cls_states).ravel()
      
    return output
  
  # Freeze/Unfreeze bert
  def freeze_bert(self):
    self.bert.requires_grad_(False)

  def unfreeze_bert(self):
    self.bert.requires_grad_(True)

  def freeze_encoderlayers(self, layers = None):
    assert layers is not None
    for l in layers:
      list(self.bert.encoder.layer)[l].requires_grad_(False)

  def unfreeze_encoderlayers(self, layers = None):
    assert layers is not None
    for l in layers:
      list(self.bert.encoder.layer)[l].requires_grad_(True)

# ==== Trainer ====
class Trainer:
   
  def __init__(self, model, device = "cuda:0"):

    # Get device
    if device == "cuda:0" and torch.cuda.is_available():
      self.device = torch.device("cuda:0")
    else:
      self.device = torch.device("cpu")
    print(f"device: {device}")

    # Put model on GPU
    self.model = model.to(device)

  def set_optimization(self, criterion_reg, criterion_rank, optimizer, metric):

    self.criterion_reg = criterion_reg.to(self.device)
    self.criterion_rank = criterion_rank.to(self.device)
    self.metric = metric.to(self.device)
    self.optimizer = optimizer

  def regression_step(self, data, scaler):

      """
      Regression step by minimizing the MSE loss
      """
      # Load single sequences to GPU
      input_ids = data["input_ids"].to(self.device)
      attention_mask = data["attention_mask"].to(self.device)
      position_ids = data["position_ids"].to(self.device)
      ph = data["pH"].to(self.device)
      tm = data["tm"].to(self.device)

      if self.settings_train["truncate"] == "split":
        n_splits = data["n_splits"]
      else:
        n_splits = None

      # Forward pass
      self.optimizer.zero_grad()
      with torch.autocast(device_type='cuda', dtype=torch.float16, 
                          enabled = True):
        tm_pred = self.model(input_ids, attention_mask, ph, 
                            n_splits = n_splits, 
                            position_ids = position_ids)

        # Compute regression loss
        loss_reg = self.criterion_reg(tm_pred, tm)
        scaler.scale(loss_reg).backward()
        scaler.step(self.optimizer)
        scaler.update()
        self.optimizer.zero_grad()
        
        # Append outputs
        self.running["loss_reg"].append(loss_reg.item())
        self.running["tm_pred"].append(tm_pred.float())
        self.running["tm_true"].append(tm)
        self.running["ids"].append(data["id"])

  def ranking_step(self, data_pairs, scaler):
    """
    Performs a ranking step by minimizing the MarginRankingLoss
    """
    # Load pairs to GPU
    input_ids_pairs = data_pairs["input_ids"].to(self.device)
    attention_mask_pairs = data_pairs["attention_mask"].to(self.device)
    ph_pairs = data_pairs["pH"].to(self.device)
    tm_pairs = data_pairs["tm"].to(self.device)
    position_ids = data_pairs["position_ids"].to(self.device)

    if self.settings_train["truncate"] == "split":
      n_splits_pairs = data_pairs["n_splits"]
    else:
      n_splits_pairs = None
    
    with torch.autocast(device_type='cuda', dtype=torch.float16, 
                        enabled = True):
      tm_pred_pairs = self.model(input_ids_pairs, attention_mask_pairs, 
                                ph_pairs, n_splits = n_splits_pairs,
                                position_ids = position_ids)
      # Split to get pairs
      tm_1 = tm_pairs[:len(tm_pairs)//2]
      tm_2 = tm_pairs[len(tm_pairs)//2:]

      tm_pred_1 = tm_pred_pairs[:len(tm_pairs)//2]
      tm_pred_2 = tm_pred_pairs[len(tm_pairs)//2:]

      rank = torch.sign(tm_1 - tm_2)

      loss_rank = self.criterion_rank(tm_pred_1, tm_pred_2, rank)

      # Backprop
      scaler.scale(loss_rank).backward()
      scaler.step(self.optimizer)
      scaler.update()
      self.optimizer.zero_grad()

      # Add loss
      self.running["loss_rank"].append(loss_rank.item())

  def create_dict_metrics(self):

        """
        Creates dictionaries for metrics
        TODO: Generic
        """
        self.running = {
        "ids": [],
        "loss_reg": [],
        "loss_rank": [],
        "tm_pred": [],
        "tm_true": []
        }
        self.out_train = {
            "Loss_reg": [],
            "Loss_rank": [],
            "SCC": [],
            "SCC_mut_avg": []
        }
        self.out_val = {
            "Loss_reg": [],
            "Loss_rank": [],
            "SCC": [],
            "SCC_mut_avg": []
        }

  def fit(self, dataset_train, datasetpairs_train = None,
          dataset_val = None, settings_train = None, settings_val = None, 
          n_epochs = 10, batch_size = 32, n_logs = 0, 
          train_rank = False, n_rank_step = 1,
          num_workers = 0, pin_memory = False, mixed_precision = True,
          debug = False):

    if settings_train is not None:
      dataset_train.settings = settings_train
      self.settings_train = settings_train
    
    if settings_val is not None:
      dataset_val.settings = settings_val
      self.settings_val = settings_val
    

    self.create_dict_metrics()

    max_scc_val = 0

    # Adding scaler 
    scaler = torch.cuda.amp.GradScaler(enabled = mixed_precision)

    # Training loop
    for i_epoch in range(n_epochs):
      
      ## Dataloader Individual Proteins
      # New batches of sequences each epoch
      groups = dataset_train.sequence_sampler(batch_size = batch_size)
      if debug:
        groups = groups[:50]
      # New dataloader for new group
      dataloader_train = torch.utils.data.DataLoader(dataset = dataset_train,
                                        batch_sampler = groups,
                                        collate_fn = dataset_train.collate)
      dataloader_train_iter = iter(dataloader_train)
      
      ## Dataloader Pairs of Protein
      if train_rank:
        # Generate batches of pairs
        groups_pairs = datasetpairs_train.pair_sampler(
            batch_size_min = batch_size, 
            n_batches_max = n_rank_step * len(dataloader_train))
        if debug:
          groups_pairs = groups_pairs[:50]
        # New dataloader for pairs
        dataloaderpairs_train = torch.utils.data.DataLoader(
            dataset = datasetpairs_train, batch_sampler = groups_pairs, 
            collate_fn = datasetpairs_train.collate)
        dataloaderpairs_iter = iter(dataloaderpairs_train)

      # Batch indices forwhich to log metrics
      list_i_logs = np.linspace(0,len(dataloader_train), 
                               n_logs)[1:].astype(int)

      for i_batch in tqdm(range(len(dataloader_train))):
        
        # Set model to train mode
        self.model.train()
        
        # Regression step
        data = next(dataloader_train_iter)
        self.regression_step(data, scaler)

        if train_rank:
          for k in range(n_rank_step):
            # Get batch of pairs
            data_pairs = next(dataloaderpairs_iter)
            # Ranking step
            self.ranking_step(data_pairs, scaler)


        if i_batch in list_i_logs:
          
          # Log metrics
          self.running["tm_pred"] = torch.cat(self.running["tm_pred"])
          self.running["tm_true"] = torch.cat(self.running["tm_true"])
          self.running["ids"] = np.concatenate(self.running["ids"])

          running_metric = self.metric(self.running["tm_pred"], 
                                       self.running["tm_true"]).item()

          log_string = (f"\n<TRAIN> Epoch: {i_epoch:<5} - "
                        f"Batch: {i_batch:<5}| "
                        f"Loss_reg: {np.mean(self.running['loss_reg']):<10.3f}")
          if train_rank:
            log_string += f"Loss_rank: {np.mean(self.running['loss_rank']):<10.3f}"
          
          log_string += (f"SCC: {100*running_metric:<8.3f} ")
          
          print(log_string)
          
          # Reset all running arrays
          for k in self.running.keys():
            self.running[k] = []
    
      # Evaluate on train set
      results_train = self.evaluate(dataset_train, settings_val, 
                                  batch_size = batch_size*1.5,
                                  evaluate_frac= 0.7,
                                  num_workers = 4,  pin_memory = True, 
                                  debug = debug)
      
      corrcoef_train = results_train["corrcoef"]
      corrcoeff_mutants_train = results_train["corrcoef_mutants"].mean()
      
      self.out_train["Loss_reg"].append(results_train['loss_reg'])
      self.out_train["Loss_rank"].append(results_train['loss_rank'])
      self.out_train["SCC"].append(corrcoef_train)
      self.out_train["SCC_mut_avg"].append(corrcoeff_mutants_train)
      
      # Evaluate on validation set
      results_val = self.evaluate(dataset_val, settings_val, 
                                  batch_size = batch_size*1.5,
                                  num_workers = 4,  pin_memory = True, 
                                  debug = debug)

      corrcoef_val = results_val["corrcoef"]
      corrcoeff_mutants_val = results_val["corrcoef_mutants"].mean()

      self.out_val["Loss_reg"].append(results_val['loss_reg'])
      self.out_val["Loss_rank"].append(results_val['loss_rank'])
      self.out_val["SCC"].append(corrcoef_val)
      self.out_val["SCC_mut_avg"].append(corrcoeff_mutants_val)

      print(f"\n<TRAIN> Epoch: {i_epoch:<5} |"
            f"Loss_reg: {results_train['loss_reg']:<10.3f} "
            f"Loss_rank: {results_train['loss_rank']:<10.3f} "
            f"SCC: {100*corrcoef_train:<8.3f}"
            f"Mut-Avg Scc: {100*corrcoeff_mutants_train:<8.3f}")

      print(f"<VALID> Epoch: {i_epoch:<5} |"
            f"Loss_reg: {results_val['loss_reg']:<10.3f} "
            f"Loss_rank: {results_val['loss_rank']:<10.3f} "
            f"SCC: {100*corrcoef_val:<8.3f}"
            f"Mut-Avg Scc: {100*corrcoeff_mutants_val:<8.3f}\n")
      
      # Save best model on validation
      if max_scc_val < corrcoef_val:
        max_scc_val = corrcoef_val
        self.best_model = copy.deepcopy(self.model)
        print("Saved as best model.\n")
            
  def predict(self, dataset, settings_val = None, 
              batch_size = 512, num_workers = 0, 
              pin_memory = False, evaluate_frac = 1, 
              mixed_precision = True, debug = False):


    # Prep dataloaders
    groups = dataset.sequence_sampler(batch_size = batch_size, shuffle = False)
    if debug:
      groups = groups[:50]

    if 0< evaluate_frac < 1:
      group_ids_sampled = np.random.choice(np.arange(len(groups)), 
                                      size = int(evaluate_frac * len(groups)))
      groups = [groups[i] for i in group_ids_sampled]

    dataloader = torch.utils.data.DataLoader(dataset = dataset,
                                        batch_sampler = groups,
                                        collate_fn = dataset.collate)
    # Set model to evaluation mode
    self.model.eval()

    # Prep prediction arrays
    tm_pred = []
    tm_true = []
    ids = []
    loss = 0
    
    # Adding scaler to 
    scaler = torch.cuda.amp.GradScaler(enabled = mixed_precision)
    
    for i_batch, data in enumerate(tqdm(dataloader)):

      # Load data to GPU
      input_ids = data["input_ids"].to(self.device)
      attention_mask = data["attention_mask"].to(self.device)
      ph_batch = data["pH"].to(self.device)
      position_ids = data["position_ids"].to(self.device)
      
      if settings_val["truncate"] == "split":
        n_splits = data["n_splits"]
      else:
        n_splits = None

      with torch.no_grad():
        with torch.autocast(device_type='cuda', dtype=torch.float16, 
                            enabled = True):
          # Forward pass
          tm_pred_batch = self.model(input_ids, attention_mask, ph_batch,
                                    n_splits = n_splits,
                                    position_ids = position_ids)

      # Append results
      tm_pred.append(tm_pred_batch.float())
      ids.append(data["id"])

    tm_pred = torch.cat(tm_pred)
    ids = np.concatenate(ids)

    return {"tm_pred": tm_pred, "ids": ids}

  def evaluate(self, dataset, settings_val = None, batch_size = 32, 
               num_workers = 0,  pin_memory = False, evaluate_frac = 1, debug = False):

    dict_pred = self.predict(dataset, settings_val, batch_size, num_workers, 
                             pin_memory, evaluate_frac = evaluate_frac, 
                             debug = debug)

    tm_pred = dict_pred["tm_pred"]
    tm_true = dataset.df.iloc[dict_pred["ids"]]["tm"].values
    tm_true = torch.from_numpy(tm_true).to(self.device)

    # Compute loss
    loss_reg = self.criterion_reg(tm_pred, tm_true).item()
    # Compute rank loss
    tm_pred_1, tm_pred_2, rank = dataset.group_mutations(dict_pred["ids"], 
                                              tm_pred)
    if tm_pred_1 is not None:
      loss_rank = self.criterion_rank(tm_pred_1.to(self.device), 
                                      tm_pred_2.to(self.device), 
                                      rank.to(self.device)).item()
    else:
      loss_rank = np.inf

    # Evaluate Spearman's Corr
    corrcoef = self.metric(tm_pred, tm_true).item()

    # Evaluate Spearman's Corr on mutants groups
    s_SCC = dataset.compute_mutation_SCC(ids = dict_pred["ids"],
                                         tm_pred = tm_pred.cpu().numpy())
    if s_SCC is None:
      s_SCC = - np.inf

    return {"tm_pred": tm_pred,
            "tm_true": tm_true,
            "ids": dict_pred["ids"],
            "loss_reg": loss_reg,
            "loss_rank": loss_rank,
            "corrcoef": corrcoef,
            "corrcoef_mutants": s_SCC}
                             